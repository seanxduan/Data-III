---
title: "Data 3 HW 2"
author: "Sean Duan"
date: "9/23/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(plyr)
library(ggplot2)
library(MASS)
library(ISLR)
library(class)
library(boot)
library(e1071)
data(Boston)
youtube<-read.csv("youtube.csv")
data(Pima.tr)
data(Pima.te)

```

# 1.

## A
```{r 1a}
set.seed(1)
x=rnorm(100)
y=x-2*x^2+rnorm (100)

```

For this formula n is 100, and p is 2.

$$x \sim N(0,1) $$
$$y = x-(2  x^2) + \epsilon  $$
## B

```{r 1b}
plot(x,y)
```
It seems like nonlinear relationship between x and y, given that the functional form of the data follows a parabola.

## C
```{r 1c}
set.seed(3)

#loocv
data_1<-as.data.frame(cbind(x,y))


cv.error=rep(0,4)
for (i in 1:4){
  glm.fit=glm(y~poly(x ,i),data=data_1)
  cv.error[i]=cv.glm(data_1 ,glm.fit)$delta [1]
}
cv.error
```
Each element in the list above is analogous to the error for the 1st, 2nd, 3rd, and 4th order polynomials respectively.

## D
```{r 1d}
set.seed(4)

#loocv
data_1<-as.data.frame(cbind(x,y))


cv.error=rep(0,4)
for (i in 1:4){
  glm.fit=glm(y~poly(x ,i),data=data_1)
  cv.error[i]=cv.glm(data_1 ,glm.fit)$delta [1]
}
cv.error
```
We see no difference with a different random seed, because no sampling is used in LOOCV. The entire data set is fit observation by observation for every element.

## E
The model with the smallest LOOCV error is our second model, the one with the 2nd order polynomial on our predictor. This is exactly what I expected, as our plot of the effects of x on y had a parabolic arc in it's form.

## F
```{r 1f}
for (i in 1:4){
  print(summary(glm(y~poly(x ,i),data=data_1)))
  }
```
We see that the exponential term up to 2 is significant in all our models, as our previous answer would lead us to expect due to the form of our graph that we diagrammed earlier. For our models, the polynomial 3 and 4 models coefficients past the 2nd exponent are not significant, as a parabolic relationship would suggest.

# 2.
## A
```{r 2a}
mean(Boston$medv)
```

## B
```{r 2b}
(sd(Boston$medv))/sqrt(length(Boston$medv))
```
Given the value of our mean, at approximately 22.5, having a standard error of .4 seems proportionally very small. Thus, we can conclude that we are fairly confident in our estimate.

## C
```{r 2c}
boot_fn<-function(data, index){
  mean(data[index])
}
boot_result<-boot(data = Boston$medv, statistic = boot_fn, R=1000)
boot_result
```
The standard error of our estimate of the mean is very slightly larger using the bootstrap. However, given the size of our mean value, and the proportional change as compared to our previous estimate of the standard error, we can conclude that there isn't a significant difference if we use the bootstrap to estimate our standard error.

## D
```{r 2d}
t.test(Boston$medv)
mean(Boston$medv)-2*0.4012719
mean(Boston$medv)+2*0.4012719
```
Our results that we obtained using the t.test function are extremely similar to our 95% confidence interval we calculated using the boostrap. This provides further confidence that our bootstrap calculation has no significant difference from our other method of estimation.

## E
```{r 2e}
median(Boston$medv)
```

## F
```{r 2f}
boot_fn2<-function(data, index){
  median(data[index])
}
boot_result2<-boot(data = Boston$medv, statistic = boot_fn2, R=1000)
boot_result2
```
The size of the standard error is similar to that which we have found earlier for the mean. This makes sense as our median estimate is less sensitive to outlier information, however, our data doesn't seem to have a great deal of outliers. Thus, it makes sense that our mean and median estimates would be similar

## G
```{r 2g}
quantile(Boston$medv, probs = .1)
```

## H
```{r 2h}
boot_fn3<-function(data, index){
  quantile(data[index], probs = .1)
}
boot_result3<-boot(data = Boston$medv, statistic = boot_fn3, R=1000)
boot_result3
```
Our estimation of standard error seems similar for the 10th percentile of our value as compared to our estimation for the standard errors of of mean and median. This lends us some confidence that our data-set is relatively uniform throughout it's entire range of recorded values.

# 3.
```{r 3}

set.seed(1)

mods=list(1,2,3,4,c(1,2),c(1,3),c(1,4),c(2,3),c(2,4),c(3,4),
          
          c(1,2,3),c(1,2,4),c(1,3,4),c(2,3,4))

cv.error=rep(0,length(mods))

for(i in 1:length(mods)){
  
  youtube_mod=youtube[,c(1,1+mods[[i]])] 
  
  glm.fit=glm(utime~.,data=youtube_mod)
  
  cv.error[i]=cv.glm(youtube_mod,glm.fit,K=5)$delta[1]
  
}

print("The cv errors for our models")
cv.error

print("the model with lowest cv error")
which.min(cv.error)
glm.fitbest=glm(utime~size+umem+OutputPixels,data=youtube)





shapiro.test(glm.fitbest$residuals)

par(mfrow=c(1,3))

boxplot(glm.fitbest$residuals)

hist(glm.fitbest$residuals,main="residuals")

qqplot(glm.fitbest$residuals,rnorm(1000),xlab="residuals",ylab="normal quantiles")
```
It seems like our "best" model under 5 fold cross validation is one that predicts upload time from size, computer memory, and output pixels.

Looking at our plot of the residuals, it seems like our residuals have a potentially Leptokurtic distribution, with a slight right skew. This violates our assumption that the residuals would follow a normal distribution without skew.

# 4.
## A
```{r 4a}
vars <- c("npreg","glu", "bp", "skin", "bmi", "ped", "age")
comb.vars <- expand.grid(vars, vars, stringsAsFactors = FALSE)
comb.vars <- comb.vars[comb.vars[,1] != comb.vars[,2],]
i.vars <- apply(comb.vars, 1, paste, collapse = "+")

cv_error<-list(NA)
cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)
modelfits<-list(NA)
for(i in 1:length(i.vars)) {
  modelformula <- paste("type ~", i.vars[i])
  modelfits[[i]] <- glm(as.formula(modelformula), family = "binomial", data = Pima.tr)
  cv_error[[i]]=cv.glm(Pima.tr,modelfits[[i]], cost = cost ,K=5)$delta[1]
}
cv_error

best_model<-modelfits[[38]]
summary(best_model)
```
The best model which we selected using 5-fold cross validation was one predicting diabetes using glucose levels and age.

My interpretation for the cost function is that it takes the absolute value of the difference between our reported outcome, and our predicted outcome. The understanding that the vector it outputs would be 0 if the guess was the exact same between our predicted and reported outcome, and 1 if the guess was the complete opposite to the predicted outcome. Thus, we are using a threshold value of greater than .5 as a binary outcome of a 'failure', wheras the threshold value lesser than .5 is a binary outcome of a 'successful' prediction.

## B
```{r 4b}
glm.probs=predict(best_model, Pima.te ,type="response")
glm.pred=rep("No" ,332)
glm.pred[glm.probs >.5]="Yes"
table(glm.pred,Pima.te$type)
mean(glm.pred==Pima.te$type)
```

## C
```{r 4c}
full_model<-glm(type~., data = Pima.tr, family = "binomial")
step_model<-stepAIC(full_model, direction = "both")
step_model_1<-stepAIC(glm(type~1, data = Pima.tr, family = "binomial"),scope=formula(full_model),direction = "forward")

best_step_model<-glm(type~glu + age +ped +bmi +ped, data = Pima.tr, family = "binomial")

glm.probs=predict(best_step_model, Pima.te ,type="response")
glm.pred=rep("No" ,332)
glm.pred[glm.probs >.5]="Yes"
table(glm.pred,Pima.te$type)
mean(glm.pred==Pima.te$type)
```
I decided to use a forward and backward stepwise algorithm using AIC as our criterion in order to decided which model is the best logistic regression classification model.

Using this model, we can generate a confusion matrix of our results, and compare it to our previous confusion matrix. Looking at our results, we have an overall lower error rate in our second model found using the stepwise principle. However, while we have a higher specificity we have a lower sensitivity.


## D
```{r 4d}
cost <- function(r,p) -2*(sum(log(p)))


modelfits<-list(NA)
for(i in 1:length(i.vars)) {
  modelformula <- paste("type ~", i.vars[i])
  modelfits[[i]] <- glm(as.formula(modelformula), family = "binomial", data = Pima.tr)
  cv_error[[i]]=cv.glm(Pima.tr,modelfits[[i]], cost = cost ,K=5)$delta[1]
}
cv_error
which.min(cv_error)

best_model2<-modelfits[[23]]
summary(best_model2)
```
I chose to use an extension of the code that was used for 4a, but instead we are using a deviance loss cost function. The results that this alternative loss function found was that the best model was one that predicted diabetes outcome from diabetes pedigree function and skin fold measurements. It is impressive to me that by merely specifying loss function, we were able to come to a different conclusion on what model was superior for our purposes.